{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Voting Classifier </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train different algorithms and will take majority of them no wighted average is called <b>hard voting classfier</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the\n",
    "best classifier in the ensemble. In fact, even if each classifier is a weak learner (mean‐\n",
    "ing it does only slightly better than random guessing), the ensemble can still be a\n",
    "strong learner (achieving high accuracy), provided there are a sufficient number of\n",
    "weak learners and they are sufficiently diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Ensemble methods work best when the predictors are as independ‐\n",
    "ent from one another as possible. One way to get diverse classifiers\n",
    "is to train them using very different algorithms. This increases the\n",
    "chance that they will make very different types of errors, improving\n",
    "the ensemble’s accuracy</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y, = iris['data'], iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn accuracy is 0.987\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "y_predict = cross_val_predict(knn_clf, X, y)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Knn accuracy is %.3f\"%accuracy_score(y, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC accuracy is 0.947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc_clf = LinearSVC(loss=\"hinge\", C=1)\n",
    "y_predict = cross_val_predict(svc_clf, X, y)\n",
    "print(\"Linear SVC accuracy is %.3f\"%accuracy_score(y, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy is 0.960\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier(max_depth=3)\n",
    "y_predict = cross_val_predict(dt_clf, X, y)\n",
    "print(\"Decision Tree accuracy is %.3f\"%accuracy_score(y, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of hard voting classfier0.973\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=3)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "svc_clf = LinearSVC(loss=\"hinge\", C=1)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vt_clf = VotingClassifier(\n",
    "    estimators = [('dt_clf',dt_clf),('knn',knn_clf),('svc', svc_clf)],\n",
    "    voting = 'hard'\n",
    ")\n",
    "y_predict = cross_val_predict(vt_clf, X, y)\n",
    "print(\"accuracy of hard voting classfier%.3f\"%accuracy_score(y, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> soft voting classfier takes weighted average of probabilities </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of soft voting classfier0.967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "dt_clf = DecisionTreeClassifier(max_depth=3)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "svc_clf = SVC(kernel='linear', C=1, probability=True)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vt_clf = VotingClassifier(\n",
    "    estimators = [('dt_clf',dt_clf),('knn',knn_clf),('svc', svc_clf)],\n",
    "    voting = 'soft'\n",
    ")\n",
    "y_predict = cross_val_predict(vt_clf, X, y)\n",
    "print(\"accuracy of soft voting classfier%.3f\"%accuracy_score(y, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont know why I less accuracy than other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Bagging and Pasting</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above we gave same training data to different classifier and took voting, but here in Bagging we will train same model but different subsamples of training data with replacement this is called Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if with out replacement this is called Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each individual\n",
    "predictor has a higher bias than if it were trained on the original training set, but\n",
    "aggregation reduces both bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ,y = breast_cancer['data'], breast_cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, n_jobs=-1, max_samples=150, bootstrap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_predict = cross_val_predict(bag_clf, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525483304042179"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>With less number of estimators lets check accuracy </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9472759226713533"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, n_jobs=-1, max_samples=150, bootstrap=True)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_predict = cross_val_predict(bag_clf, X, y, cv=10)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349736379613357"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=3, n_jobs=-1, max_samples=150, bootstrap=True)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_predict = cross_val_predict(bag_clf, X, y, cv=10)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Out of Bag Evaluation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BaggingClassifier, we take random samples of 70% and we try to build estimators each, intersting thing here is you can evaluate your classifier with remiaining 30% of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Value of Out of Bag Evaluation is average of all Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy is 0.960\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, n_jobs=-1, max_samples=150, bootstrap=True, oob_score=True)\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_predict = cross_val_predict(bag_clf, X, y, cv=10)\n",
    "from sklearn.metrics import accuracy_score\n",
    "cross_validation_accuaracy = accuracy_score(y, y_predict)\n",
    "print(\"Cross Validation Accuracy is %.3f\"%cross_validation_accuaracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oob score is 0.951\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, n_jobs=-1, max_samples=150, bootstrap=True, oob_score=True)\n",
    "bag_clf.fit(X, y)\n",
    "oob_score = bag_clf.oob_score_\n",
    "print(\"oob score is %.3f\"%oob_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOB score is some what similar to accuracy, when you increase n_estimator, they will close than this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Patches and Random Subspaces</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Classifier Support Sampling Features as well like Sampling Training Instances, using two hyper paramters\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<i>\n",
    "max_features like max_samples\n",
    "<br>\n",
    "bootstrap_features like bootstrap\n",
    "</i>\n",
    "<br>\n",
    "<br>\n",
    "If you random sampling and random features this is called <b>Random Pathes </b>\n",
    "<br><br>\n",
    "If you do only random features this is called <b> Random Subspaces </b>\n",
    "<br><br>\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for\n",
    "a lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595782073813708"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, n_jobs=-1, max_samples=300, bootstrap=True, oob_score=True, max_features=10, bootstrap_features=True)\n",
    "y_predict = cross_val_predict(bag_clf, X, y, cv=3)\n",
    "accuracy_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feauture Importance Random Forest..</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         mean radius 4.179\n",
      "        mean texture 1.530\n",
      "      mean perimeter 5.491\n",
      "           mean area 3.661\n",
      "     mean smoothness 0.589\n",
      "    mean compactness 1.460\n",
      "      mean concavity 4.618\n",
      " mean concave points 7.466\n",
      "       mean symmetry 0.371\n",
      "mean fractal dimension 0.315\n",
      "        radius error 1.952\n",
      "       texture error 0.463\n",
      "     perimeter error 1.479\n",
      "          area error 3.173\n",
      "    smoothness error 0.468\n",
      "   compactness error 0.632\n",
      "     concavity error 0.491\n",
      "concave points error 0.446\n",
      "      symmetry error 0.453\n",
      "fractal dimension error 0.475\n",
      "        worst radius 12.529\n",
      "       worst texture 1.806\n",
      "     worst perimeter 12.104\n",
      "          worst area 12.761\n",
      "    worst smoothness 1.410\n",
      "   worst compactness 1.818\n",
      "     worst concavity 2.265\n",
      "worst concave points 13.641\n",
      "      worst symmetry 1.302\n",
      "worst fractal dimension 0.651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators = 150, n_jobs = -1)\n",
    "random_forest.fit(X, y)\n",
    "for name, score in zip(breast_cancer['feature_names'], random_forest.feature_importances_):\n",
    "    print(\"%20s %.3f\"%(name, score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DESCR', 'target', 'data', 'target_names', 'feature_names'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris_X, iris_y = iris['data'], iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm) 10.361\n",
      "    sepal width (cm) 2.303\n",
      "   petal length (cm) 45.972\n",
      "    petal width (cm) 41.364\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators = 150, n_jobs = -1)\n",
    "random_forest.fit(iris_X, iris_y)\n",
    "for name, score in zip(iris['feature_names'], random_forest.feature_importances_):\n",
    "    print(\"%20s %.3f\"%(name, score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Boosting </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is ensemble learning that combines weak learners in to a strong learner\n",
    "<br><br>\n",
    "Basic Idea is training serveral predictors sequentially, each correcting its predecessor<br>\n",
    "<br>\n",
    "1. Ada Boosting(Adaptive)\n",
    "2. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Boosting is sequential, It can't be parallelized like bagging, so boosting doesn't scale well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ada Boost </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\n",
    "stands for Stagewise Additive Modeling using a Multiclass Exponential loss function).\n",
    "When there are just two classes, SAMME is equivalent to AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn can use a variant of <b>SAMME</b> called <b>SAMME.R</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_boost = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm = \"SAMME.R\", learning_rate=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9718804920913884"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict  = cross_val_predict(ada_boost, X, y)\n",
    "accuracy_score(y, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Ada boost got very good Accuracy </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your ada boost is overfitting, then regularize by decreasing number of estimators or more strongly regularizing base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gradient Boosting..</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally gradient boosting will be used in regression task so ignoring for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Stack </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model stack is very very Important, lot of people use this in Kaggle\n",
    "<h5> Few tips </h5>\n",
    "<ol>\n",
    "    <li>Pick base estimator with varied structure</li>\n",
    "    <li>Pick meta estimator that can handle high correlations</li>\n",
    "    <li>Simple meta estomators can aid interpretability(L2 penalized logistic regression..)</li>\n",
    "    <li>Using continous outputs from base estimators unlike classification values 1/0 using methods like <b>predict_proba, decision_function.</b>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Does this Work </h5>\n",
    "1. Asymptotically, use stacked model estimator is no worse than your base best estimator\n",
    "2. Anecdotally, we have found stacked models make great push-button estimators, especially for our 'medium' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=3gpf1lGwecA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
